{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "X_train_query: str = '''\n",
    "SELECT \n",
    "    \"speaker\"                as \"speaker\",\n",
    "    \"statement\"              as \"headline\",\n",
    "    \"fullText_based_content\" as \"body\"\n",
    "FROM \"fnn_train\";\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test\n",
    "X_test_query: str = '''\n",
    "SELECT \n",
    "    \"speaker\"                as \"speaker\",\n",
    "    \"statement\"              as \"headline\",\n",
    "    \"fullText_based_content\" as \"body\"\n",
    "FROM \"fnn_test\";\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train\n",
    "Y_train_query: str = '''\n",
    "SELECT \n",
    "    \"label_fnn\" as \"target\" \n",
    "FROM \"fnn_train\";\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_test\n",
    "Y_test_query: str = '''\n",
    "SELECT \n",
    "    \"label_fnn\" as \"target\" \n",
    "FROM \"fnn_test\";\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"data.sqlite3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_sql(X_train_query, con=conn)\n",
    "X_test = pd.read_sql(X_test_query, con=conn)\n",
    "y_train = pd.read_sql(Y_train_query, con=conn)\n",
    "y_test = pd.read_sql(Y_test_query, con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(\n",
    "    [\n",
    "        \"speaker\", \n",
    "        \"headline\"\n",
    "    ], \n",
    "    axis=1, \n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.drop(\n",
    "    [\n",
    "        \"speaker\", \n",
    "        \"headline\"\n",
    "    ], \n",
    "    axis=1, \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encode of the Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['target'] = y_train['target'].astype('category')\n",
    "y_test['target'] = y_test['target'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['target'] = y_train['target'].str.replace('real', '0')\n",
    "y_train['target'] = y_train['target'].str.replace('fake', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['target'] = y_test['target'].str.replace('real', '0')\n",
    "y_test['target'] = y_test['target'].str.replace('fake', '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Lowecase and Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['body'] = X_train['body'].str.lower().str.replace('[^\\w\\s]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['body'] = X_test['body'].str.lower().str.replace('[^\\w\\s]', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (Stopwords, Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk import corpus\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_row: int = X_train.shape[0] + X_test.shape[0]\n",
    "progress_bar = tqdm(total=total_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Feature_Engineering(txt: str, stopwords: bool, lemmatization: bool):\n",
    "    global total_row, progress_bar\n",
    "    \n",
    "    # Tokenizing\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "    txt = tokenizer.tokenize(txt.strip())\n",
    "    \n",
    "    # Stopwards\n",
    "    if stopwords:\n",
    "        stopwords_lst = corpus.stopwords.words(\"english\")\n",
    "        txt = (word for word in txt if word not in stopwords_lst)\n",
    "        \n",
    "    # Lemmatization\n",
    "    if lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        txt = (lemmatizer.lemmatize(word) for word in txt)\n",
    "    \n",
    "    # Making sentence\n",
    "    txt = ' '.join(txt)\n",
    "    \n",
    "    # Updating Progressbar\n",
    "    progress_bar.update(n=1)\n",
    "    \n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_Train_Feature_Engineering():\n",
    "    global X_train\n",
    "    X_train['body'] = X_train['body'].apply(\n",
    "        lambda text: Feature_Engineering(\n",
    "            txt=text,\n",
    "            stopwords=True,\n",
    "            lemmatization=False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_Test_Feature_Engineering():\n",
    "    global X_test\n",
    "    X_test['body'] = X_test['body'].apply(\n",
    "        lambda text: Feature_Engineering(\n",
    "            txt = text,\n",
    "            stopwords=True,\n",
    "            lemmatization=False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_Train_Feature_Engineering_Thread = threading.Thread(target=X_Train_Feature_Engineering)\n",
    "# X_Test_Feature_Engineering_Thread = threading.Thread(target=X_Test_Feature_Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_Feature_Engineering()\n",
    "X_Test_Feature_Engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_Train_Feature_Engineering_Thread.start()\n",
    "# X_Test_Feature_Engineering_Thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_Train_Feature_Engineering_Thread.join()\n",
    "# X_Test_Feature_Engineering_Thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del total_row, progress_bar, X_Train_Feature_Engineering_Thread, X_Test_Feature_Engineering_Thread\n",
    "del total_row, progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra usefull columns creating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['word_count'] = X_train[\"body\"].apply(\n",
    "    lambda x: len(str(x).split())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['char_count'] = X_train[\"body\"].apply(\n",
    "    lambda x: sum(len(word) for word in str(x).split())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train['sentence_count'] = X_train[\"body\"].apply(lambda x: len(str(x).split(\".\")))  # 1\n",
    "X_train['avg_word_length'] = X_train['char_count'] / X_train['word_count']\n",
    "X_train['avg_sentence_length'] = X_train['word_count'] # / X_train['sentence_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['word_count'] = X_test[\"body\"].apply(\n",
    "    lambda x: len(str(x).split())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['char_count'] = X_test[\"body\"].apply(\n",
    "    lambda x: sum(len(word) for word in str(x).split())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test['sentence_count'] = X_test[\"body\"].apply(lambda x: len(str(x).split(\".\")))  # 1\n",
    "X_test['avg_word_length'] = X_test['char_count'] / X_test['word_count']\n",
    "X_test['avg_sentence_length'] = X_test['word_count'] # / X_test['sentence_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping copy of the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ORIG, y_train_ORIG = X_train.copy(), y_train.copy()\n",
    "X_test_ORIG, y_test_ORIG = X_test.copy(), y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat(\n",
    "    [\n",
    "        X_train_ORIG, \n",
    "        y_train_ORIG\n",
    "    ],\n",
    "    axis=1, \n",
    "    ignore_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat(\n",
    "    [\n",
    "        X_test_ORIG, \n",
    "        y_test_ORIG\n",
    "    ],\n",
    "    axis=1,\n",
    "    ignore_index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('processed_data.xlsx') as writer: \n",
    "    df_train.to_excel(\n",
    "        excel_writer=writer, \n",
    "        sheet_name='Train', \n",
    "        header=True, \n",
    "        index=False\n",
    "    )\n",
    "    df_test.to_excel(\n",
    "        excel_writer=writer, \n",
    "        sheet_name='Test', \n",
    "        header=True, \n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Count of Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.set(\n",
    "    style='whitegrid',\n",
    "    color_codes=True,\n",
    ")\n",
    "sns.countplot(\n",
    "    x='target',\n",
    "    data=df_train,\n",
    "    hue='target'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(X_train.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(X_train.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = vectorizer.transform(X_test.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping copy of vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.DataFrame(\n",
    "    tuple(vectorizer.vocabulary_.items()), \n",
    "    columns = ['word', 'id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.to_excel(\"vocabulary_data.xlsx\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Select_Best_Features(y: pd.core.series.Series, p_value_limit: int, feature_selector: str):\n",
    "    global vectorizer\n",
    "\n",
    "    if feature_selector.lower() == 'chi2':\n",
    "        _, p = feature_selection.chi2(X_train, y)\n",
    "    elif feature_selector.lower() == 'anova':\n",
    "        _, p = feature_selection.f_classif(X_train, y)\n",
    "    \n",
    "    X_features = pd.DataFrame()\n",
    "    X_features = X_features.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                'feature': vectorizer.get_feature_names(),\n",
    "                'score': 1 - p,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return X_features[X_features['score'] > p_value_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = Select_Best_Features(\n",
    "    y=y_train['target'],\n",
    "    p_value_limit=0.95,\n",
    "    feature_selector='anova'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features.to_excel(\"processed_vocabulary_data.xlsx\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    vocabulary=X_features[\"feature\"].unique().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(X_train_ORIG.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(X_train_ORIG.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(X_test_ORIG.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping copy of vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.DataFrame(\n",
    "    tuple(vectorizer.vocabulary_.items()), \n",
    "    columns = ['word', 'id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.to_excel(\"vocabulary_data_2.xlsx\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import metrics, naive_bayes, pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(\n",
    "    booster = 'gbtree',\n",
    "    max_depth=5,\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.01,\n",
    "    importance_type='gain',\n",
    "    random_state= 0,\n",
    "    n_jobs=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train_ORIG.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true=y_test_ORIG.target, y_pred=prediction)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "visualizer = confusion_matrix(\n",
    "    classifier,\n",
    "    X_train, y_train_ORIG.target, X_test, y_test_ORIG.target,\n",
    "    classes=['Fake', 'Real'],\n",
    "    cmap='PuBu',\n",
    "    is_fitted=True\n",
    ")\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"Accuracy: {}%\".format(round(accuracy_score(y_test_ORIG, prediction) * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        y_true=y_test_ORIG.target,\n",
    "        y_pred=prediction,\n",
    "        target_names=['Fake', 'Real'],\n",
    "        zero_division='warn',\n",
    "        digits=5\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
